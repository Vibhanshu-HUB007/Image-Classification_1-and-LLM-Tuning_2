{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b254779-5a22-47bc-bd7f-34a170496c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory not found. Creating sample data...\n",
      "Sample data created in 'sample_data' directory\n",
      "Using device: cuda\n",
      "Classes: ['folderA', 'folderB']\n",
      "Train samples: 100\n",
      "Val samples: 40\n",
      "============================================================\n",
      "TRAINING MODEL 1: ResNet50\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/vibhanshu92/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ResNet50...\n",
      "Epoch 1/10, Batch 0/4, Loss: 0.6877\n",
      "Epoch 1/10:\n",
      "  Train Loss: 0.5910\n",
      "  Val Loss: 0.0673\n",
      "  Val Accuracy: 1.0000\n",
      "  New best model saved! Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 2/10, Batch 0/4, Loss: 0.0062\n",
      "Epoch 2/10:\n",
      "  Train Loss: 0.0644\n",
      "  Val Loss: 0.0115\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 3/10, Batch 0/4, Loss: 0.0068\n",
      "Epoch 3/10:\n",
      "  Train Loss: 0.0067\n",
      "  Val Loss: 0.0001\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 4/10, Batch 0/4, Loss: 0.0017\n",
      "Epoch 4/10:\n",
      "  Train Loss: 0.0013\n",
      "  Val Loss: 0.0000\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 5/10, Batch 0/4, Loss: 0.0005\n",
      "Epoch 5/10:\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0000\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 6/10, Batch 0/4, Loss: 0.0002\n",
      "Epoch 6/10:\n",
      "  Train Loss: 0.0058\n",
      "  Val Loss: 0.0000\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 7/10, Batch 0/4, Loss: 0.0001\n",
      "Epoch 7/10:\n",
      "  Train Loss: 0.0326\n",
      "  Val Loss: 0.0000\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 8/10, Batch 0/4, Loss: 0.0036\n",
      "Epoch 8/10:\n",
      "  Train Loss: 0.4700\n",
      "  Val Loss: 0.0000\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 9/10, Batch 0/4, Loss: 0.0078\n",
      "Epoch 9/10:\n",
      "  Train Loss: 0.2079\n",
      "  Val Loss: 0.0001\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 10/10, Batch 0/4, Loss: 0.0222\n",
      "Epoch 10/10:\n",
      "  Train Loss: 0.0129\n",
      "  Val Loss: 0.0036\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "============================================================\n",
      "TRAINING MODEL 2: EfficientNet-B0\n",
      "============================================================\n",
      "\n",
      "Training EfficientNet...\n",
      "Epoch 1/10, Batch 0/4, Loss: 0.6942\n",
      "Epoch 1/10:\n",
      "  Train Loss: 0.6121\n",
      "  Val Loss: 0.5004\n",
      "  Val Accuracy: 1.0000\n",
      "  New best model saved! Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 2/10, Batch 0/4, Loss: 0.1757\n",
      "Epoch 2/10:\n",
      "  Train Loss: 0.1141\n",
      "  Val Loss: 0.1687\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 3/10, Batch 0/4, Loss: 0.0444\n",
      "Epoch 3/10:\n",
      "  Train Loss: 0.0717\n",
      "  Val Loss: 0.0152\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 4/10, Batch 0/4, Loss: 0.0262\n",
      "Epoch 4/10:\n",
      "  Train Loss: 0.3370\n",
      "  Val Loss: 0.0016\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 5/10, Batch 0/4, Loss: 0.0019\n",
      "Epoch 5/10:\n",
      "  Train Loss: 0.0271\n",
      "  Val Loss: 0.0013\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 6/10, Batch 0/4, Loss: 0.0039\n",
      "Epoch 6/10:\n",
      "  Train Loss: 2.1976\n",
      "  Val Loss: 0.0020\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 7/10, Batch 0/4, Loss: 0.0088\n",
      "Epoch 7/10:\n",
      "  Train Loss: 0.0184\n",
      "  Val Loss: 0.0018\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 8/10, Batch 0/4, Loss: 0.0076\n",
      "Epoch 8/10:\n",
      "  Train Loss: 0.0438\n",
      "  Val Loss: 0.0007\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 9/10, Batch 0/4, Loss: 0.0176\n",
      "Epoch 9/10:\n",
      "  Train Loss: 0.0329\n",
      "  Val Loss: 0.0004\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "Epoch 10/10, Batch 0/4, Loss: 0.0304\n",
      "Epoch 10/10:\n",
      "  Train Loss: 0.0242\n",
      "  Val Loss: 0.0002\n",
      "  Val Accuracy: 1.0000\n",
      "--------------------------------------------------\n",
      "============================================================\n",
      "TRAINING MODEL 3: YOLOv8 Classification\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-cls.pt to 'yolov8n-cls.pt': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.31M/5.31M [00:00<00:00, 7.09MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.174 ðŸš€ Python-3.9.19 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1650 Ti, 3708MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=sample_data, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/classify/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/train... found 100 images in 2 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid... found 40 images in 2 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    332802  ultralytics.nn.modules.head.Classify         [256, 2]                      \n",
      "YOLOv8n-cls summary: 56 layers, 1,440,850 parameters, 1,440,850 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "WARNING âš ï¸ \u001b[34m\u001b[1mAMP: \u001b[0mchecks failed âŒ. AMP training on NVIDIA GeForce GTX 1650 Ti GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1062.8Â±385.7 MB/s, size: 22.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/train... 100 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 4369.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1053.7Â±579.3 MB/s, size: 22.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid... 40 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 1319.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      1.11G     0.7513          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  7.75it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30      1.11G     0.6678          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 23.58it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 86.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      1.11G     0.5997          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 24.90it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 80.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.525          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30      1.11G     0.4506          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 27.37it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 74.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.525          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      1.11G     0.4135          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 27.83it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 84.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.575          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      1.11G     0.4146          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 23.75it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 87.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.575          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/home/vibhanshu92/.config/Ultralytics/Arial.ttf':   8%|â–Š         | 64.0k/755k [00:00<00:01, 654kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/home/vibhanshu92/.config/Ultralytics/Arial.ttf': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 3.43MB/s]\n",
      "       7/30      1.11G     0.2538          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 19.73it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 84.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.95          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30      1.11G     0.2669          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 28.18it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 88.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30      1.11G     0.2249          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 26.65it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 82.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30      1.11G     0.3235          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 23.92it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 83.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30      1.11G     0.1947          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 28.70it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 85.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      1.11G     0.2566          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 28.52it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 78.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30      1.11G     0.1668          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 31.16it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 81.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30      1.11G     0.1579          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 26.06it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 77.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30      1.11G     0.2042          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 28.43it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 79.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30      1.11G     0.2198          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 24.47it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 78.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30      1.11G     0.3566          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 30.81it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 75.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30      1.11G     0.5792          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 29.16it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 77.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30      1.11G     0.2412          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 28.30it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 73.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.95          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30      1.11G     0.2295          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 28.64it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 78.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.575          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30      1.11G     0.1936          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 14.71it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 79.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30      1.11G     0.1416          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 29.28it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 86.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30      1.11G     0.1461          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 29.64it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 84.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30      1.11G      0.175          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 29.30it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 81.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30      1.11G     0.1638          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 30.72it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 82.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30      1.11G     0.1403          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 29.13it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 84.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30      1.11G     0.2022          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 28.21it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 89.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30      1.11G      0.274          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 30.22it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 77.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30      1.11G     0.1136          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 28.07it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 84.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30      1.11G     0.4133          4        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 28.88it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 76.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.5          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 0.004 hours.\n",
      "Optimizer stripped from runs/classify/train/weights/last.pt, 3.0MB\n",
      "Optimizer stripped from runs/classify/train/weights/best.pt, 3.0MB\n",
      "\n",
      "Validating runs/classify/train/weights/best.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.9.19 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1650 Ti, 3708MiB)\n",
      "YOLOv8n-cls summary (fused): 30 layers, 1,437,442 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/train... found 100 images in 2 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid... found 40 images in 2 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n",
      "Speed: 0.9ms preprocess, 1.1ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/train\u001b[0m\n",
      "============================================================\n",
      "INFERENCE SPEED BENCHMARKING\n",
      "============================================================\n",
      "ResNet50 - Average inference time: 9.14ms Â± 1.31ms\n",
      "EfficientNet - Average inference time: 12.16ms Â± 1.30ms\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 8.6ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.8ms\n",
      "Speed: 2.3ms preprocess, 4.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.0ms\n",
      "Speed: 1.3ms preprocess, 4.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 13.1ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.9ms\n",
      "Speed: 1.7ms preprocess, 3.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.4ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.0ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.0ms\n",
      "Speed: 1.3ms preprocess, 4.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.1ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.9ms\n",
      "Speed: 1.2ms preprocess, 3.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.0ms\n",
      "Speed: 1.6ms preprocess, 4.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.4ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.4ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.1ms\n",
      "Speed: 1.3ms preprocess, 4.1ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.2ms\n",
      "Speed: 1.5ms preprocess, 4.2ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.9ms\n",
      "Speed: 1.3ms preprocess, 3.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.2ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.2ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.2ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.2ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.2ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.2ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.3ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.2ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.3ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.2ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.2ms\n",
      "Speed: 1.4ms preprocess, 4.2ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.1ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.4ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.3ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.4ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.2ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.0ms\n",
      "Speed: 1.6ms preprocess, 4.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.4ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.5ms\n",
      "Speed: 1.2ms preprocess, 3.5ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.9ms\n",
      "Speed: 1.3ms preprocess, 3.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.3ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.5ms\n",
      "Speed: 1.3ms preprocess, 3.5ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.3ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.3ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.3ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.4ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.0ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.5ms\n",
      "Speed: 1.2ms preprocess, 3.5ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.3ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.2ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.2ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.2ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.2ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.9ms\n",
      "Speed: 2.4ms preprocess, 3.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.2ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.9ms\n",
      "Speed: 1.3ms preprocess, 3.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.2ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.9ms\n",
      "Speed: 1.2ms preprocess, 3.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.2ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.4ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.5ms\n",
      "Speed: 1.1ms preprocess, 3.5ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.2ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.6ms\n",
      "Speed: 1.3ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "image 1/1 /home/vibhanshu92/Downloads/Task_1_Image_Classification/sample_data/valid/folderA/image_000.jpg: 224x224 folderA 0.63, folderB 0.37, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "YOLO - Average inference time: 10.75ms Â± 8.25ms\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "ResNet50: 9.14ms - âœ“ (<20ms)\n",
      "EfficientNet: 12.16ms - âœ“ (<20ms)\n",
      "YOLO: 10.75ms - âœ“ (<20ms)\n",
      "\n",
      "Results saved to 'model_performance_summary.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import timm\n",
    "from ultralytics import YOLO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ImageClassificationPipeline:\n",
    "    def __init__(self, data_dir, num_classes=2, batch_size=32, num_epochs=10):\n",
    "        self.data_dir = data_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Data transforms\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.setup_data_loaders()\n",
    "        \n",
    "    def setup_data_loaders(self):\n",
    "        \"\"\"Setup train and validation data loaders\"\"\"\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            root=os.path.join(self.data_dir, 'train'),\n",
    "            transform=self.train_transform\n",
    "        )\n",
    "        \n",
    "        val_dataset = datasets.ImageFolder(\n",
    "            root=os.path.join(self.data_dir, 'valid'),\n",
    "            transform=self.val_transform\n",
    "        )\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, \n",
    "            shuffle=True, num_workers=4\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset, batch_size=self.batch_size, \n",
    "            shuffle=False, num_workers=4\n",
    "        )\n",
    "        \n",
    "        self.class_names = train_dataset.classes\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "        print(f\"Train samples: {len(train_dataset)}\")\n",
    "        print(f\"Val samples: {len(val_dataset)}\")\n",
    "\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        # Freeze early layers\n",
    "        for param in list(self.resnet.parameters())[:-20]:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace final layer\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.resnet.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "class EfficientNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EfficientNetClassifier, self).__init__()\n",
    "        self.efficientnet = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "        # Freeze early layers\n",
    "        for param in list(self.efficientnet.parameters())[:-30]:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Replace classifier\n",
    "        self.efficientnet.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(self.efficientnet.classifier.in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)\n",
    "\n",
    "class YOLOClassifier:\n",
    "    def __init__(self, data_dir, num_classes):\n",
    "        self.data_dir = data_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.model = None\n",
    "        \n",
    "    def prepare_yolo_data(self):\n",
    "        \"\"\"Convert folder structure to YOLO format\"\"\"\n",
    "        # This is a simplified version - in practice you'd need proper YOLO dataset structure\n",
    "        print(\"Preparing YOLO data structure...\")\n",
    "        # For demonstration, we'll use YOLOv8 classification mode\n",
    "        \n",
    "    def train(self, epochs=50):\n",
    "        \"\"\"Train YOLO model\"\"\"\n",
    "        # Initialize YOLOv8 classification model\n",
    "        self.model = YOLO('yolov8n-cls.pt')  # nano version for speed\n",
    "        \n",
    "        # Train the model\n",
    "        results = self.model.train(\n",
    "            data=self.data_dir,\n",
    "            epochs=epochs,\n",
    "            imgsz=224,\n",
    "            batch=16,\n",
    "            device=0 if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        return results\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        if self.model:\n",
    "            self.model.save(path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load saved model\"\"\"\n",
    "        self.model = YOLO(path)\n",
    "    \n",
    "    def predict(self, image_path):\n",
    "        \"\"\"Run inference\"\"\"\n",
    "        if self.model:\n",
    "            results = self.model(image_path)\n",
    "            return results\n",
    "\n",
    "def train_pytorch_model(model, train_loader, val_loader, num_epochs, device, model_name):\n",
    "    \"\"\"Generic training function for PyTorch models\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                pred = output.argmax(dim=1)\n",
    "                val_predictions.extend(pred.cpu().numpy())\n",
    "                val_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = accuracy_score(val_targets, val_predictions)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {avg_train_loss:.4f}')\n",
    "        print(f'  Val Loss: {avg_val_loss:.4f}')\n",
    "        print(f'  Val Accuracy: {val_acc:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'best_{model_name.lower()}_model.pth')\n",
    "            print(f'  New best model saved! Accuracy: {best_val_acc:.4f}')\n",
    "        \n",
    "        print('-' * 50)\n",
    "    \n",
    "    return model, train_losses, val_losses, val_accuracies\n",
    "\n",
    "def benchmark_inference_speed(model, test_image_path, device, num_runs=100):\n",
    "    \"\"\"Benchmark inference speed\"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load and preprocess test image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(test_image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Warm up\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(input_tensor)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            _ = model(input_tensor)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    return avg_time, std_time\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample data structure for testing\"\"\"\n",
    "    import os\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create directory structure\n",
    "    os.makedirs(\"sample_data/train/folderA\", exist_ok=True)\n",
    "    os.makedirs(\"sample_data/train/folderB\", exist_ok=True)\n",
    "    os.makedirs(\"sample_data/valid/folderA\", exist_ok=True)\n",
    "    os.makedirs(\"sample_data/valid/folderB\", exist_ok=True)\n",
    "    \n",
    "    # Generate sample images\n",
    "    for split in [\"train\", \"valid\"]:\n",
    "        for folder in [\"folderA\", \"folderB\"]:\n",
    "            num_images = 50 if split == \"train\" else 20\n",
    "            for i in range(num_images):\n",
    "                # Generate random image\n",
    "                if folder == \"folderA\":\n",
    "                    # Create images with more red\n",
    "                    img_array = np.random.randint(100, 255, (224, 224, 3), dtype=np.uint8)\n",
    "                    img_array[:, :, 0] = np.random.randint(150, 255, (224, 224))  # More red\n",
    "                else:\n",
    "                    # Create images with more blue\n",
    "                    img_array = np.random.randint(100, 255, (224, 224, 3), dtype=np.uint8)\n",
    "                    img_array[:, :, 2] = np.random.randint(150, 255, (224, 224))  # More blue\n",
    "                \n",
    "                img = Image.fromarray(img_array)\n",
    "                img.save(f\"sample_data/{split}/{folder}/image_{i:03d}.jpg\")\n",
    "    \n",
    "    print(\"Sample data created in 'sample_data' directory\")\n",
    "    return \"sample_data\"\n",
    "\n",
    "def main():\n",
    "    # Check if data directory exists, if not create sample data\n",
    "    DATA_DIR = \"sample_data\"\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        print(\"Data directory not found. Creating sample data...\")\n",
    "        DATA_DIR = create_sample_data()\n",
    "    \n",
    "    # Configuration\n",
    "    NUM_CLASSES = 2  # folderA and folderB\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 10  # Reduced for faster training with sample data\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = ImageClassificationPipeline(\n",
    "        data_dir=DATA_DIR,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_epochs=NUM_EPOCHS\n",
    "    )\n",
    "    \n",
    "    device = pipeline.device\n",
    "    \n",
    "    # Model 1: ResNet50\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING MODEL 1: ResNet50\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resnet_model = ResNetClassifier(NUM_CLASSES)\n",
    "    resnet_model, resnet_train_losses, resnet_val_losses, resnet_val_accs = train_pytorch_model(\n",
    "        resnet_model, pipeline.train_loader, pipeline.val_loader, \n",
    "        NUM_EPOCHS, device, \"ResNet50\"\n",
    "    )\n",
    "    \n",
    "    # Model 2: EfficientNet\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING MODEL 2: EfficientNet-B0\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    efficientnet_model = EfficientNetClassifier(NUM_CLASSES)\n",
    "    efficientnet_model, eff_train_losses, eff_val_losses, eff_val_accs = train_pytorch_model(\n",
    "        efficientnet_model, pipeline.train_loader, pipeline.val_loader, \n",
    "        NUM_EPOCHS, device, \"EfficientNet\"\n",
    "    )\n",
    "    \n",
    "    # Model 3: YOLO (for classification)\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING MODEL 3: YOLOv8 Classification\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    yolo_classifier = YOLOClassifier(DATA_DIR, NUM_CLASSES)\n",
    "    yolo_results = yolo_classifier.train(epochs=30)\n",
    "    yolo_classifier.save_model('best_yolo_classifier.pt')\n",
    "    \n",
    "    # Save all models\n",
    "    torch.save(resnet_model.state_dict(), 'resnet50_classifier.pth')\n",
    "    torch.save(efficientnet_model.state_dict(), 'efficientnet_classifier.pth')\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"INFERENCE SPEED BENCHMARKING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test image path - use first validation image\n",
    "    test_image_path = os.path.join(DATA_DIR, \"valid\", \"folderA\", \"image_000.jpg\")\n",
    "    \n",
    "    if os.path.exists(test_image_path):\n",
    "        # Benchmark ResNet\n",
    "        resnet_avg_time, resnet_std = benchmark_inference_speed(\n",
    "            resnet_model, test_image_path, device\n",
    "        )\n",
    "        print(f\"ResNet50 - Average inference time: {resnet_avg_time:.2f}ms Â± {resnet_std:.2f}ms\")\n",
    "        \n",
    "        # Benchmark EfficientNet\n",
    "        eff_avg_time, eff_std = benchmark_inference_speed(\n",
    "            efficientnet_model, test_image_path, device\n",
    "        )\n",
    "        print(f\"EfficientNet - Average inference time: {eff_avg_time:.2f}ms Â± {eff_std:.2f}ms\")\n",
    "        \n",
    "        # Benchmark YOLO\n",
    "        yolo_times = []\n",
    "        for _ in range(100):\n",
    "            start_time = time.time()\n",
    "            _ = yolo_classifier.predict(test_image_path)\n",
    "            end_time = time.time()\n",
    "            yolo_times.append((end_time - start_time) * 1000)\n",
    "        \n",
    "        yolo_avg_time = np.mean(yolo_times)\n",
    "        yolo_std = np.std(yolo_times)\n",
    "        print(f\"YOLO - Average inference time: {yolo_avg_time:.2f}ms Â± {yolo_std:.2f}ms\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ResNet50: {resnet_avg_time:.2f}ms - {'âœ“' if resnet_avg_time < 20 else 'âœ—'} (<20ms)\")\n",
    "        print(f\"EfficientNet: {eff_avg_time:.2f}ms - {'âœ“' if eff_avg_time < 20 else 'âœ—'} (<20ms)\")\n",
    "        print(f\"YOLO: {yolo_avg_time:.2f}ms - {'âœ“' if yolo_avg_time < 20 else 'âœ—'} (<20ms)\")\n",
    "        \n",
    "        # Performance summary\n",
    "        results_summary = {\n",
    "            \"models\": {\n",
    "                \"resnet50\": {\n",
    "                    \"best_val_accuracy\": max(resnet_val_accs),\n",
    "                    \"avg_inference_time_ms\": resnet_avg_time,\n",
    "                    \"model_path\": \"resnet50_classifier.pth\"\n",
    "                },\n",
    "                \"efficientnet\": {\n",
    "                    \"best_val_accuracy\": max(eff_val_accs),\n",
    "                    \"avg_inference_time_ms\": eff_avg_time,\n",
    "                    \"model_path\": \"efficientnet_classifier.pth\"\n",
    "                },\n",
    "                \"yolo\": {\n",
    "                    \"avg_inference_time_ms\": yolo_avg_time,\n",
    "                    \"model_path\": \"best_yolo_classifier.pt\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open('model_performance_summary.json', 'w') as f:\n",
    "            json.dump(results_summary, f, indent=2)\n",
    "        \n",
    "        print(\"\\nResults saved to 'model_performance_summary.json'\")\n",
    "    else:\n",
    "        print(f\"Test image not found at {test_image_path}\")\n",
    "        print(\"Please update the test_image_path variable with a valid image path\")\n",
    "\n",
    "def load_and_inference_example():\n",
    "    \"\"\"Example of loading saved models and running inference\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load ResNet model\n",
    "    resnet_model = ResNetClassifier(num_classes=2)\n",
    "    resnet_model.load_state_dict(torch.load('resnet50_classifier.pth', map_location=device))\n",
    "    resnet_model.eval()\n",
    "    \n",
    "    # Load EfficientNet model\n",
    "    efficientnet_model = EfficientNetClassifier(num_classes=2)\n",
    "    efficientnet_model.load_state_dict(torch.load('efficientnet_classifier.pth', map_location=device))\n",
    "    efficientnet_model.eval()\n",
    "    \n",
    "    # Load YOLO model\n",
    "    yolo_model = YOLO('best_yolo_classifier.pt')\n",
    "    \n",
    "    # Example inference\n",
    "    test_image_path = \"path/to/test/image.jpg\"\n",
    "    \n",
    "    if os.path.exists(test_image_path):\n",
    "        # Preprocess image for PyTorch models\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        image = Image.open(test_image_path).convert('RGB')\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # ResNet inference\n",
    "        with torch.no_grad():\n",
    "            resnet_output = resnet_model(input_tensor)\n",
    "            resnet_pred = torch.softmax(resnet_output, dim=1)\n",
    "        \n",
    "        # EfficientNet inference\n",
    "        with torch.no_grad():\n",
    "            eff_output = efficientnet_model(input_tensor)\n",
    "            eff_pred = torch.softmax(eff_output, dim=1)\n",
    "        \n",
    "        # YOLO inference\n",
    "        yolo_results = yolo_model(test_image_path)\n",
    "        \n",
    "        print(\"Inference Results:\")\n",
    "        print(f\"ResNet50 predictions: {resnet_pred}\")\n",
    "        print(f\"EfficientNet predictions: {eff_pred}\")\n",
    "        print(f\"YOLO results: {yolo_results}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # Uncomment to run inference example\n",
    "    # load_and_inference_eximport os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import timm\n",
    "from ultralytics import YOLO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ImageClassificationPipeline:\n",
    "    def __init__(self, data_dir, num_classes=2, batch_size=32, num_epochs=10):\n",
    "        self.data_dir = data_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Data transforms\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.setup_data_loaders()\n",
    "        \n",
    "    def setup_data_loaders(self):\n",
    "        \"\"\"Setup train and validation data loaders\"\"\"\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            root=os.path.join(self.data_dir, 'train'),\n",
    "            transform=self.train_transform\n",
    "        )\n",
    "        \n",
    "        val_dataset = datasets.ImageFolder(\n",
    "            root=os.path.join(self.data_dir, 'valid'),\n",
    "            transform=self.val_transform\n",
    "        )\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, \n",
    "            shuffle=True, num_workers=4\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset, batch_size=self.batch_size, \n",
    "            shuffle=False, num_workers=4\n",
    "        )\n",
    "        \n",
    "        self.class_names = train_dataset.classes\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "        print(f\"Train samples: {len(train_dataset)}\")\n",
    "        print(f\"Val samples: {len(val_dataset)}\")\n",
    "\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        # Freeze early layers\n",
    "        for param in list(self.resnet.parameters())[:-20]:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace final layer\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.resnet.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "class EfficientNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EfficientNetClassifier, self).__init__()\n",
    "        self.efficientnet = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "        # Freeze early layers\n",
    "        for param in list(self.efficientnet.parameters())[:-30]:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Replace classifier\n",
    "        self.efficientnet.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(self.efficientnet.classifier.in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)\n",
    "\n",
    "class YOLOClassifier:\n",
    "    def __init__(self, data_dir, num_classes):\n",
    "        self.data_dir = data_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.model = None\n",
    "        \n",
    "    def prepare_yolo_data(self):\n",
    "        \"\"\"Convert folder structure to YOLO format\"\"\"\n",
    "        # This is a simplified version - in practice you'd need proper YOLO dataset structure\n",
    "        print(\"Preparing YOLO data structure...\")\n",
    "        # For demonstration, we'll use YOLOv8 classification mode\n",
    "        \n",
    "    def train(self, epochs=50):\n",
    "        \"\"\"Train YOLO model\"\"\"\n",
    "        # Initialize YOLOv8 classification model\n",
    "        self.model = YOLO('yolov8n-cls.pt')  # nano version for speed\n",
    "        \n",
    "        # Train the model\n",
    "        results = self.model.train(\n",
    "            data=self.data_dir,\n",
    "            epochs=epochs,\n",
    "            imgsz=224,\n",
    "            batch=16,\n",
    "            device=0 if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        return results\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        if self.model:\n",
    "            self.model.save(path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load saved model\"\"\"\n",
    "        self.model = YOLO(path)\n",
    "    \n",
    "    def predict(self, image_path):\n",
    "        \"\"\"Run inference\"\"\"\n",
    "        if self.model:\n",
    "            results = self.model(image_path)\n",
    "            return results\n",
    "\n",
    "def train_pytorch_model(model, train_loader, val_loader, num_epochs, device, model_name):\n",
    "    \"\"\"Generic training function for PyTorch models\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                pred = output.argmax(dim=1)\n",
    "                val_predictions.extend(pred.cpu().numpy())\n",
    "                val_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = accuracy_score(val_targets, val_predictions)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {avg_train_loss:.4f}')\n",
    "        print(f'  Val Loss: {avg_val_loss:.4f}')\n",
    "        print(f'  Val Accuracy: {val_acc:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'best_{model_name.lower()}_model.pth')\n",
    "            print(f'  New best model saved! Accuracy: {best_val_acc:.4f}')\n",
    "        \n",
    "        print('-' * 50)\n",
    "    \n",
    "    return model, train_losses, val_losses, val_accuracies\n",
    "\n",
    "def benchmark_inference_speed(model, test_image_path, device, num_runs=100):\n",
    "    \"\"\"Benchmark inference speed\"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load and preprocess test image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(test_image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Warm up\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(input_tensor)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            _ = model(input_tensor)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    return avg_time, std_time\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    DATA_DIR = \"path/to/your/data\"  # Update this path\n",
    "    NUM_CLASSES = 2  # folderA and folderB\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 15\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = ImageClassificationPipeline(\n",
    "        data_dir=DATA_DIR,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_epochs=NUM_EPOCHS\n",
    "    )\n",
    "    \n",
    "    device = pipeline.device\n",
    "    \n",
    "    # Model 1: ResNet50\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING MODEL 1: ResNet50\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resnet_model = ResNetClassifier(NUM_CLASSES)\n",
    "    resnet_model, resnet_train_losses, resnet_val_losses, resnet_val_accs = train_pytorch_model(\n",
    "        resnet_model, pipeline.train_loader, pipeline.val_loader, \n",
    "        NUM_EPOCHS, device, \"ResNet50\"\n",
    "    )\n",
    "    \n",
    "    # Model 2: EfficientNet\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING MODEL 2: EfficientNet-B0\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    efficientnet_model = EfficientNetClassifier(NUM_CLASSES)\n",
    "    efficientnet_model, eff_train_losses, eff_val_losses, eff_val_accs = train_pytorch_model(\n",
    "        efficientnet_model, pipeline.train_loader, pipeline.val_loader, \n",
    "        NUM_EPOCHS, device, \"EfficientNet\"\n",
    "    )\n",
    "    \n",
    "    # Model 3: YOLO (for classification)\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING MODEL 3: YOLOv8 Classification\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    yolo_classifier = YOLOClassifier(DATA_DIR, NUM_CLASSES)\n",
    "    yolo_results = yolo_classifier.train(epochs=30)\n",
    "    yolo_classifier.save_model('best_yolo_classifier.pt')\n",
    "    \n",
    "    # Save all models\n",
    "    torch.save(resnet_model.state_dict(), 'resnet50_classifier.pth')\n",
    "    torch.save(efficientnet_model.state_dict(), 'efficientnet_classifier.pth')\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"INFERENCE SPEED BENCHMARKING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test image path (update this)\n",
    "    test_image_path = \"path/to/test/image.jpg\"\n",
    "    \n",
    "    if os.path.exists(test_image_path):\n",
    "        # Benchmark ResNet\n",
    "        resnet_avg_time, resnet_std = benchmark_inference_speed(\n",
    "            resnet_model, test_image_path, device\n",
    "        )\n",
    "        print(f\"ResNet50 - Average inference time: {resnet_avg_time:.2f}ms Â± {resnet_std:.2f}ms\")\n",
    "        \n",
    "        # Benchmark EfficientNet\n",
    "        eff_avg_time, eff_std = benchmark_inference_speed(\n",
    "            efficientnet_model, test_image_path, device\n",
    "        )\n",
    "        print(f\"EfficientNet - Average inference time: {eff_avg_time:.2f}ms Â± {eff_std:.2f}ms\")\n",
    "        \n",
    "        # Benchmark YOLO\n",
    "        yolo_times = []\n",
    "        for _ in range(100):\n",
    "            start_time = time.time()\n",
    "            _ = yolo_classifier.predict(test_image_path)\n",
    "            end_time = time.time()\n",
    "            yolo_times.append((end_time - start_time) * 1000)\n",
    "        \n",
    "        yolo_avg_time = np.mean(yolo_times)\n",
    "        yolo_std = np.std(yolo_times)\n",
    "        print(f\"YOLO - Average inference time: {yolo_avg_time:.2f}ms Â± {yolo_std:.2f}ms\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ResNet50: {resnet_avg_time:.2f}ms - {'âœ“' if resnet_avg_time < 20 else 'âœ—'} (<20ms)\")\n",
    "        print(f\"EfficientNet: {eff_avg_time:.2f}ms - {'âœ“' if eff_avg_time < 20 else 'âœ—'} (<20ms)\")\n",
    "        print(f\"YOLO: {yolo_avg_time:.2f}ms - {'âœ“' if yolo_avg_time < 20 else 'âœ—'} (<20ms)\")\n",
    "        \n",
    "        # Performance summary\n",
    "        results_summary = {\n",
    "            \"models\": {\n",
    "                \"resnet50\": {\n",
    "                    \"best_val_accuracy\": max(resnet_val_accs),\n",
    "                    \"avg_inference_time_ms\": resnet_avg_time,\n",
    "                    \"model_path\": \"resnet50_classifier.pth\"\n",
    "                },\n",
    "                \"efficientnet\": {\n",
    "                    \"best_val_accuracy\": max(eff_val_accs),\n",
    "                    \"avg_inference_time_ms\": eff_avg_time,\n",
    "                    \"model_path\": \"efficientnet_classifier.pth\"\n",
    "                },\n",
    "                \"yolo\": {\n",
    "                    \"avg_inference_time_ms\": yolo_avg_time,\n",
    "                    \"model_path\": \"best_yolo_classifier.pt\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open('model_performance_summary.json', 'w') as f:\n",
    "            json.dump(results_summary, f, indent=2)\n",
    "        \n",
    "        print(\"\\nResults saved to 'model_performance_summary.json'\")\n",
    "    else:\n",
    "        print(f\"Test image not found at {test_image_path}\")\n",
    "        print(\"Please update the test_image_path variable with a valid image path\")\n",
    "\n",
    "def load_and_inference_example():\n",
    "    \"\"\"Example of loading saved models and running inference\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load ResNet model\n",
    "    resnet_model = ResNetClassifier(num_classes=2)\n",
    "    resnet_model.load_state_dict(torch.load('resnet50_classifier.pth', map_location=device))\n",
    "    resnet_model.eval()\n",
    "    \n",
    "    # Load EfficientNet model\n",
    "    efficientnet_model = EfficientNetClassifier(num_classes=2)\n",
    "    efficientnet_model.load_state_dict(torch.load('efficientnet_classifier.pth', map_location=device))\n",
    "    efficientnet_model.eval()\n",
    "    \n",
    "    # Load YOLO model\n",
    "    yolo_model = YOLO('best_yolo_classifier.pt')\n",
    "    \n",
    "    # Example inference\n",
    "    test_image_path = \"path/to/test/image.jpg\"\n",
    "    \n",
    "    if os.path.exists(test_image_path):\n",
    "        # Preprocess image for PyTorch models\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        image = Image.open(test_image_path).convert('RGB')\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # ResNet inference\n",
    "        with torch.no_grad():\n",
    "            resnet_output = resnet_model(input_tensor)\n",
    "            resnet_pred = torch.softmax(resnet_output, dim=1)\n",
    "        \n",
    "        # EfficientNet inference\n",
    "        with torch.no_grad():\n",
    "            eff_output = efficientnet_model(input_tensor)\n",
    "            eff_pred = torch.softmax(eff_output, dim=1)\n",
    "        \n",
    "        # YOLO inference\n",
    "        yolo_results = yolo_model(test_image_path)\n",
    "        \n",
    "        print(\"Inference Results:\")\n",
    "        print(f\"ResNet50 predictions: {resnet_pred}\")\n",
    "        print(f\"EfficientNet predictions: {eff_pred}\")\n",
    "        print(f\"YOLO results: {yolo_results}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # Uncomment to run inference example\n",
    "    # load_and_inference_example()ample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
